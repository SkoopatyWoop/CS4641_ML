{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos.exceptions import CosmosResourceExistsError\n",
    "import azure.cosmos.cosmos_client as cosmos_client\n",
    "from azure.cosmos.partition_key import PartitionKey\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from BagOfWords import BOW\n",
    "from nb import NaiveBayes\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Initialization & Naive Bayes\n",
    "Here, we build our dataset using Bag of Words to initialize our 'X' and 'Y' arrays. Then, we analyze it using our Naive Bayes net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database with id 'ethiclassifier' initialized\n",
      "Container with id 'data' initialized\n"
     ]
    }
   ],
   "source": [
    "config = dotenv_values('.env')\n",
    "client = cosmos_client.CosmosClient(\n",
    "    config['AZURE_SQL_HOST'],\n",
    "    {\n",
    "        'masterKey': config['AZURE_SQL_MASTER_KEY']\n",
    "    },\n",
    "    user_agent=\"CosmosDBPythonQuickstart\",\n",
    "    user_agent_overwrite=True\n",
    ")\n",
    "db = client.create_database_if_not_exists(id=config['AZURE_SQL_DATABASE_ID'])\n",
    "print('Database with id \\'{0}\\' initialized'.format(config['AZURE_SQL_DATABASE_ID']))\n",
    "container = db.create_container_if_not_exists(\n",
    "    id=config['AZURE_SQL_CONTAINER_ID'],\n",
    "    partition_key=PartitionKey(path='/ethical_tag'),\n",
    "    offer_throughput=1000\n",
    ")\n",
    "nb = NaiveBayes()\n",
    "print('Container with id \\'{0}\\' initialized'.format(config['AZURE_SQL_CONTAINER_ID']))\n",
    "\n",
    "items = container.read_all_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database with id 'ethiclassifier' initialized\n",
      "Container with id 'data' initialized\n"
     ]
    }
   ],
   "source": [
    "config = dotenv_values('.env')\n",
    "client = cosmos_client.CosmosClient(\n",
    "    config['AZURE_SQL_HOST'],\n",
    "    {\n",
    "        'masterKey': config['AZURE_SQL_MASTER_KEY']\n",
    "    },\n",
    "    user_agent=\"CosmosDBPythonQuickstart\",\n",
    "    user_agent_overwrite=True\n",
    ")\n",
    "db = client.create_database_if_not_exists(id=config['AZURE_SQL_DATABASE_ID'])\n",
    "print('Database with id \\'{0}\\' initialized'.format(config['AZURE_SQL_DATABASE_ID']))\n",
    "container = db.create_container_if_not_exists(\n",
    "    id=config['AZURE_SQL_CONTAINER_ID'],\n",
    "    partition_key=PartitionKey(path='/ethical_tag'),\n",
    "    offer_throughput=1000\n",
    ")\n",
    "nb = NaiveBayes()\n",
    "print('Container with id \\'{0}\\' initialized'.format(config['AZURE_SQL_CONTAINER_ID']))\n",
    "\n",
    "items = container.read_all_items()\n",
    "items = list(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethicals_full = [item for item in items if item['ethical_tag']]\n",
    "ethicals = ethicals_full[:2472]\n",
    "\n",
    "unethicals = [item for item in items if not item['ethical_tag']]\n",
    "\n",
    "# Build the dataset with the data entries we get from the 'items' list.\n",
    "dataset = ethicals + unethicals\n",
    "np.random.seed(69420)\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someone taking parking spaces crowded street refuses reverse feet politely asked so False\n",
      "0.8068756319514662 0.7917087967644085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = list(map(lambda x: x[\"text\"], dataset))\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "pipe = Pipeline([('vec', vectorizer), ('logreg', LogisticRegression(penalty='l2'))])\n",
    "\n",
    "pipe2 = Pipeline([('vec', vectorizer), ('cnb', ComplementNB())])\n",
    "\n",
    "y = list(map(lambda x: x[\"ethical_tag\"], dataset))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "acc1 = pipe.score(X_test, y_test)\n",
    "cv1 = np.mean(cross_val_score(pipe, X, y, cv=5))\n",
    "pred_logreg = pipe.predict(X_test)\n",
    "\n",
    "pipe2.fit(X_train, y_train)\n",
    "acc2 = pipe2.score(X_test, y_test)\n",
    "cv2 = np.mean(cross_val_score(pipe2, X, y, cv=5))\n",
    "pred_cnb = pipe2.predict(X_test)\n",
    "\n",
    "print(acc1, acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "  'alpha': np.linspace(0.01, 0.05, 2),\n",
    "  'hidden_layer_sizes': [(10,10,5)],\n",
    "  'learning_rate_init': np.linspace(0.1, 0.5, 2),\n",
    "  'activation': ['tanh', 'relu'],\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=50)\n",
    "model = GridSearchCV(mlp, grid_params)\n",
    "\n",
    "pipeNN = Pipeline([('vec', vectorizer), \n",
    "                        ('nn', model)])\n",
    "\n",
    "# cv2 = np.mean(cross_val_score(pipeNN, X, y, cv=5))\n",
    "# print(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[:int((0.8)*len(dataset))]\n",
    "test = dataset[int((0.8)*len(dataset)):]\n",
    "\n",
    "bow_train = BOW(train)\n",
    "trainX = bow_train.X\n",
    "trainY = bow_train.y\n",
    "\n",
    "bow_test = BOW(test)\n",
    "testX = bow_test.X\n",
    "testY = bow_test.y\n",
    "# print(np.stack([testY, pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error percent: 0.4418604651162791\n",
      "mse: 0.4418604651162791\n",
      "0.6638169663873458\n"
     ]
    }
   ],
   "source": [
    "cnb = ComplementNB()\n",
    "cnb.fit(trainX, trainY)\n",
    "\n",
    "pred = cnb.predict(testX)\n",
    "diffs = testY ^ pred\n",
    "error_pct = sum(diffs) / len(pred)\n",
    "print(\"error percent: \" + str(error_pct))\n",
    "mse = mean_squared_error(1 * testY, 1 * pred)\n",
    "print(\"mse: \" + str(mse))\n",
    "\n",
    "scores = cross_val_score(cnb, np.concatenate((trainX, testX)), np.concatenate((trainY, testY)), cv=5)\n",
    "avg_acc = np.mean(scores)\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989,) (989,)\n",
      "error percent: 0.4418604651162791\n",
      "mse: 0.4418604651162791\n",
      "0.7040756008400093\n"
     ]
    }
   ],
   "source": [
    "# we used l2 penalty as we have colinear / codependent features \n",
    "logreg = LogisticRegression(penalty='l2')\n",
    "logreg.fit(trainX, trainY)\n",
    "print(pred.shape, testY.shape)\n",
    "diffs = testY ^ pred\n",
    "error_pct = sum(diffs) / len(pred)\n",
    "print(\"error percent: \" + str(error_pct))\n",
    "mse = mean_squared_error(1 * testY, 1 * pred)\n",
    "print(\"mse: \" + str(mse))\n",
    "\n",
    "scores = cross_val_score(logreg, np.concatenate((trainX, testX)), np.concatenate((trainY, testY)), cv=5)\n",
    "avg_acc = np.mean(scores)\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "ulpt = \"Flying on your employer's dime? Book an expensive flight, then call the airline later to change for a cheaper fare (ie earlier/later the same day). Ask for the travel credit to be sent to your personal email. Use it later for personal travel. Your employer will not know.\"\n",
    "lpt = \"I'm 43. By your late 20's/early 30's, make sure physical fitness becomes an absolute top priority. I started a dedicated fitness regimen when I was 28 to improve my odds with a girl. Didn't work on the girl. What did work was that the routine stuck. Now pushing my mid-forties, I can't believe where I am physically compared to many others my age. Also scary is how they regard physical deterioration as an inevitability. It isn't. Get started now. It will be one of the greatest gifts you'll ever give yourself.\"\n",
    "\n",
    "# ulpt = \"donate to charities\"\n",
    "\n",
    "data = [ulpt, lpt]\n",
    "print(pipe.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs & Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('مساعدة', 13348), ('خط', 13347), ('باللغة', 13346), ('العربية', 13345), ('الأزمات', 13344), ('ɥsᴉlƃuǝ', 13343), ('écoute', 13342), ('zz', 13341), ('zoom', 13340), ('zone', 13339), ('zoes', 13338), ('zodiacs', 13337), ('zirconia', 13336), ('ziptie', 13335), ('zippers', 13334), ('ziplock', 13333), ('ziploc', 13332), ('zip', 13331), ('zimmers', 13330), ('zillow', 13329), ('zest', 13328), ('zeroed', 13327), ('zero', 13326), ('zelle', 13325), ('zealand', 13324), ('zatarains', 13323), ('yummy', 13322), ('yummier', 13321), ('yule', 13320), ('yt', 13319), ('ysk', 13318), ('yrs', 13317), ('ypu', 13316), ('youwhich', 13315), ('youve', 13314), ('youtubevanced', 13313), ('youtubes', 13312), ('youtubers', 13311), ('youtuber', 13310), ('youtube', 13309), ('youthspecific', 13308), ('youth', 13307), ('youre', 13306), ('youngsters', 13305), ('younger', 13304), ('young', 13303), ('youll', 13302), ('youhttps', 13301), ('york', 13300), ('yoinked', 13299), ('yoghurt', 13298), ('yoga', 13297), ('yob', 13296), ('yo', 13295), ('ymmv', 13294), ('ymca', 13293), ('yjcqcdejas', 13292), ('yield', 13291), ('yesterday', 13290), ('yeses', 13289), ('yes', 13288), ('yep', 13287), ('yelp', 13286), ('yellow', 13285), ('yelling', 13284), ('yell', 13283), ('yeeyee', 13282), ('yeet', 13281), ('years', 13280), ('yearround', 13279), ('yearly', 13278), ('year', 13277), ('yeah', 13276), ('yawn', 13275), ('yarn', 13274), ('yard', 13273), ('yall', 13272), ('ya', 13271), ('xz', 13270), ('xyz', 13269), ('xxxxxxxxxxx', 13268), ('xxxxxxx', 13267), ('xxxx', 13266), ('xxx', 13265), ('xx', 13264), ('xs', 13263), ('xpost', 13262), ('xpm', 13261), ('xo', 13260), ('xmas', 13259), ('xfinity', 13258), ('xenon', 13257), ('xd', 13256), ('xbox', 13255), ('xb', 13254), ('xanex', 13253), ('xanax', 13252), ('wwwzelfmoordbe', 13251), ('wwwyourlifeyourvoiceorg', 13250), ('wwwwhoint', 13249)]\n"
     ]
    }
   ],
   "source": [
    "from plots import plot\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# plotter = plot(y_test, pred_logreg)\n",
    "# plotter.fpr_fnr()\n",
    "\n",
    "items = vectorizer.vocabulary_.items()\n",
    "sorted_arr = sorted(items, key = lambda x: x[1], reverse = True)\n",
    "top100 = sorted_arr[:100]\n",
    "print(top100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1   0\n",
      "0  NaN NaN\n",
      "1  NaN NaN\n",
      "2  NaN NaN\n",
      "3  NaN NaN\n",
      "4  NaN NaN\n",
      "..  ..  ..\n",
      "95 NaN NaN\n",
      "96 NaN NaN\n",
      "97 NaN NaN\n",
      "98 NaN NaN\n",
      "99 NaN NaN\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_45912\\3502510072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Use the function with the rome_corpus and our mask to create word cloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mgenerate_better_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rome, Italy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreddit_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_45912\\3502510072.py\u001b[0m in \u001b[0;36mgenerate_better_wordcloud\u001b[1;34m(data, title, mask)\u001b[0m\n\u001b[0;32m     12\u001b[0m                       \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                       \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                       collocations=True).generate_from_frequencies(data)\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krish\\Dropbox (GaTech)\\Documents\\Semesters\\Spring 2022\\CS 4641\\HWs\\CS4641_ML\\venv\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \"\"\"\n\u001b[0;32m    400\u001b[0m         \u001b[1;31m# make sure frequencies are sorted and normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n",
      "\u001b[1;32mc:\\Users\\krish\\Dropbox (GaTech)\\Documents\\Semesters\\Spring 2022\\CS 4641\\HWs\\CS4641_ML\\venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m         raise ValueError(\n\u001b[1;32m-> 1538\u001b[1;33m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1539\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "reddit_mask = np.array(Image.open('../images/reddit_logo.png'))\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib as plt\n",
    "\n",
    "# from https://towardsdatascience.com/how-to-create-beautiful-word-clouds-in-python-cfcf85141214\n",
    "\n",
    "# A similar function, but using the mask\n",
    "def generate_better_wordcloud(data, title, mask=None):\n",
    "    cloud = WordCloud(scale=3,\n",
    "                      max_words=150,\n",
    "                      colormap='RdYlGn',\n",
    "                      mask=mask,\n",
    "                      background_color='white',\n",
    "                      collocations=True).generate_from_frequencies(data)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "corpus = pd.DataFrame(top100)\n",
    "print(corpus)\n",
    "columns_titles = [\"1\",\"0\"]\n",
    "corpus = corpus.reindex(columns=columns_titles)\n",
    "# Use the function with the rome_corpus and our mask to create word cloud     \n",
    "generate_better_wordcloud(corpus, 'Rome, Italy', mask=reddit_mask)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "937fb0e1e5b009dfc6768b5f878866cb2bb20d525391c20752f70e57d1cef599"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
